In this section,  we present three different frameworks  deployed on HPC
systems  to  measure the  power  consumption  and  performance of  the
baseline execution.

%\subsection{Framework description}
%\label{subsec:3.1}

\subsection{E3METER metering products}
Supercomputer  clusters considered  for our  experiments at  the Swiss
National Supercomputing Center of  ETH Zurich (CSCS) are equipped with
E3METER Intelligent  Power Strips (IPS)  and Monitors (IPM)  which are
high   quality   electricity  meters   released   by  Riedo   Networks
(\url{http://riedonetworks.com/}),  that  enable  to monitor  and  log
power  consumption of  the  IT infrastructure  as  well as  constantly
analyze  line  voltage,  current,  power-factor,  frequency  with  1\%
accuracy.   Using reliable  narrowband  powerline communication  (PLC)
technology,  all metering  and power  quality data  from each  IPS are
centrally collected by the E3METER Data Concentrator, via the existing
power cables thus  avoiding the need for extra  cabling.  This data is
made  available  via SNMP,  HTTP,  TELNET  through  the built-in  Fast
Ethernet  port.   Time  synchronization  is guaranteed  by  using  NTP
servers.   Measured data  is accessed  through the  open  source Cacti
software including  the E3METER  Cacti Plugin to  scan the  entire PLC
network and monitor  in real-time the power usage  of individual rack,
recorded in 5 minutes interval periods.

\textbf{MALOSSI: I MOVED THIS PART HERE, SINCE IT IS RELATED TO CSCS MACHINES: IT NEEDS TO BE INTEGRATED IN THE TEXT ABOVE}
Energy  consumption of  CSCS  clusters is
assessed  by sampling  the instantaneous  peak power  during execution
which  is then  averaged  and multiplied  by  the time-to-solution  to
determine energy-to-solution.

\subsection{University of Hamburg}
To assess the  performance and the energy efficiency  of COSMO-ART, we
employ   a    version   of   the    integrated   framework   presented
in~\cite{energy13} that works in  combination with Extrae and Paraver,
which are profiling/tracing and visualization tools, respectively.

%The left part of the \vref{fig:Lustre} offers a graphical representation of
%the Lustre architecture; the right depicts the tracing and profiling framework.
To use our  approach, COSMO-ART is compiled using  the Extrae compiler
wrappers,  which  automatically instrument  the  Fortran  code of  the
model. Next, COSMO-ART  is run on the nodes,  thus dissipating certain
amount  of power.   These  nodes are  connected  to power  measurement
devices that account for the dissipated power/consumed energy and send
the power  data to  the tracing server.  The client,  meanwhile, sends
start/stop  primitives  in  order  to  gather  captured  data  by  the
wattmeters onto  the tracing server,  where an instance of  the \pmlib
server is running.

Once COSMO-ART run is finished, a file containing the power profile is
created using the power data  received from the tracing server and the
instrumentation post-processing generates the performance trace files.
All  these files  are  combined then  in  Paraver which  allows us  to
visualize the performance trace and the power profile of COSMO-ART all
together.

%Once COSMO-ART run is finished, the VampirTrace \pmlib plugin receives
%the  power   data  from  the  tracing   server.   The  instrumentation
%post-processing generates  the performance trace files  and the \pmlib
%plugin inserts the power data into them.

%In  addition  to the  power  measurements,  we  also account  for  the
%resource utilization values  of the nodes: CPU load,  memory usage and
%storage device utilization. % and network utilization.

%We  run special  \pmlib  server  instances on  the  server nodes  that
%retrieve these  values from the \texttt{proc}  file system (leveraging
%the  \texttt{psutil} Python library).   Thus, \pmlib  plugin instances
%running  with the  instrumented  application connect  with the  \pmlib
%servers.   Finally,   using  the   Vampir   visualization  tool,   the
%power-performance traces  can be easily  analyzed through a  series of
%plots and statistics.

\subsection{IBM BG/Q integrated power measurement system}
The IBM Blue~Gene/Q~(BG/Q) supercomputer is equipped with internal hardware sensors
that provide 8~power measures for each node board (32~node cards, 512~cores), including total power, chip power, DDR~DIMM power, and network power.
Therefore, no additional hardware tools are required to obtain power measurements.
In order to instrument the code and collect power measurements in a file we use the MonEQ library~\cite{SeanWallace2013},
which provides a simple interface to start and stop power measurement in specific regions of the code.
Energy is computed a posteriori as the product of average power and time-to-solution.
