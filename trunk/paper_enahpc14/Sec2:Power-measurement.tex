\subsection{Hardware description}
\label{subsec:2.1}
While hardware  platforms mature  and are replaced,  we have  chosen a
state-of-the-art  Intel's third-generation  Core (aka  ``Ivy Bridge'')
processing platform (called ``Monch'')  for our power measurements, as
it  is slated  to stay  in service  without hardware  upgrade  for the
duration  of  the Exa2Green  project.   In  principle  at least,  this
architecture could be recreated or found in an identical configuration
beyond the lifetime of the project.  Given that the baseline benchmark
can    be    reproduced    within    an   expected    variance    (see
subsection~\ref{subsec:1.2}), and that  the baseline run configuration
can be used in all future versions of the code, a fair comparison will
be made between the baseline  and the milestone versions of COSMO-ART.
In  this   study,  a  complementary   energy-to-solution  benchmarking
comparison is carried out on  the ``Pilatus'' cluster based on Intel's
previous-generation  ``Sandy  Bridge'' processors,  known  to be  more
power consuming.

\subsubsection{Monch (CSCS - ETH Zurich)}
Monch, which is installed  at the Swiss National Supercomputing Center
(CSCS)  of the ETH  Zurich, is  a 10  rack NEC-provided  (LX-2400) and
dual-socket Intel-based cluster, utilised from people that are part of
the   Swiss  Platform   for  Advanced   Scientific   Computing  (PASC,
\url{http://www.pasc-ch.org/}).   It  is   composed  of  312  standard
compute  nodes,  24  large-memory  compute nodes  and  24  huge-memory
compute nodes.   Each standard compute node comprises  2 Intel 10-core
Ivy Bridge EP E5-2660v2 processors operating at 2.2 GHz, offering 32GB
of DDR3 1600MHz  RAM, themselves connected by a  high speed Infiniband
network,  based on Mellanox  SX6036 managed  FDR (Fourteen  Data Rate)
switches, with a 56 Gb/s speed.

\subsubsection{Pilatus (CSCS - ETH Zurich)}
Pilatus is a dual-socket Intel  Sandy-Bridge based cluster used as Piz
Daint pre-post processing cluster.  It is composed of 42 compute nodes
with  a total  number of  1344 cores  in hyper-threading  and  2688 GB
aggregate memory and has 2  high-speed interconnects based on FDR: the
first is  dedicated to the MPI  traffic and the second  to the storage
high  speed traffic.  The  2 login  nodes  and the  42 computes  nodes
consists in  11 twin-pair Intel  E5-Series DALCO r2264i4t  2U scalable
compute  modules.  Each  module contains  4 compute  nodes based  on 2
Intel  8-core Intel  Xeon  E5-2670 processors  operating  at 2.6  GHz,
offering  64GB of  DDR3 1600MHz  RAM, themselves  connected by  a high
speed  Infiniband  network,  based  on  Mellanox  SX6036  managed  FDR
(Fourteen Data Rate) switches, with a 56 Gb/s speed.

\subsection{Power-Performance measurement framework}
\label{subsec:2.2}

\subsubsection{CSCS - ETH Zurich}
E3METER  Intelligent Power Strips  (IPS) and  Monitors (IPM)  are high
quality electricity meters which enable to monitor and optimize energy
consumption of datacenters or large facilities.  A central data logger
- the  E3METER Data Concentrator  - aggregates  all measured  data and
makes them available in various forms (SNMP, Web, etc). Data transfers
between E3METER  IPS and E3METER Concentrator  use reliable narrowband
powerline  communication (PLC)  technology which  avoids the  need for
extra cabling.  It has  one internal temperature sensor. Two dedicated
extension  ports  can be  used  to  measure  external temperature  and
humidity  through E3METER  remote sensors.  It  features non-intrusive
voltage, current and frequency measurement through current transformer
with an accuracy of 1\%.

\subsubsection{University of Hamburg}
To assess the  performance and the energy efficiency  of COSMO-ART, we
employ   a    version   of   the    integrated   framework   presented
in~\cite{energy13}  that  works in  combination  with VampirTrace  and
Vampir,   which  are   profiling/tracing   and  visualization   tools,
respectively.
%The left part of the \vref{fig:Lustre} offers a graphical representation of
%the Lustre architecture; the right depicts the tracing and profiling framework.
To  use our  approach,  COSMO-ART is  compiled  using the  VampirTrace
compiler wrappers, which automatically  instrument the Fortran code of
the  model. Next,  COSMO-ART is  run  on the  nodes, thus  dissipating
certain  amount of  power.  The  server nodes  are connected  to power
measurement  devices that  account for  the  dissipated power/consumed
energy and  send the power data  to the tracing  server.  The attached
VampirTrace \pmlib plugin employs the client API that sends start/stop
primitives in order to gather captured data by the wattmeters onto the
tracing server,  where an  instance of the  \pmlib server  is running.
Once COSMO-ART run is finished, the VampirTrace \pmlib plugin receives
the  power   data  from  the  tracing   server.   The  instrumentation
post-processing generates  the performance trace files  and the \pmlib
plugin inserts the power data into them.

In  addition  to the  power  measurements,  we  also account  for  the
resource utilization values  of the nodes: CPU load,  memory usage and
storage device utilization. % and network utilization.  

We  run special  \pmlib  server  instances on  the  server nodes  that
retrieve these  values from the \texttt{proc}  file system (leveraging
the  \texttt{psutil} Python library).   Thus, \pmlib  plugin instances
running  with the  instrumented  application connect  with the  \pmlib
servers.   Finally,   using  the   Vampir   visualization  tool,   the
power-performance traces  can be easily  analyzed through a  series of
plots and statistics.
