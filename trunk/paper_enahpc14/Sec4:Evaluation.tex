In  this section,  we  describe a  performance  and energy  efficiency
evaluation  of  different  achitectures  when  running  the  COSMO-ART
baseline.  We  start by  specifying the metrics used to analyse the results on all platforms.
Then, we detail the environment  setup gathering the considered HPC systems,
the  software  environment and  the  run  configuration.  Finally,  we
discuss benchmark  results and  power-performance traces of  the model
system.

\subsection{Measurement methodology}
\label{subsec:4.1}
We  approach  the  assessment  of  the energy  footprint  and  overall
performance    of    COSMO-ART    with    two    important    metrics:
\textit{time-to-solution}       and       \textit{energy-to-solution}.
Time-to-solution  refers   to  the  total  wall  clock   time  of  the
application execution time. Energy-to-solution is the amount of energy
spent  to achieve  results.  Whenever possible, multiple production
runs of COSMO-ART were  performed to illustrate the reproducibility of
the baseline, and quantify  the significant uncertainties in the power
measurement, as dictated by the available technology.


\subsection{Sofware environment}
\label{subsec:4.2}
INT2LM  and the  COSMO-ART model  are  implemented in  Fortran 90  for
distributed  memory  parallel  computers  using  the  Message  Passing
Interface (MPI)  and are purely  MPI-parallel.

\textbf{MALOSSI: Here I suggest to create small subsections for each machine to avoid confusion especially because we need to put details of 4 different machines\ldots}

The software  stack on
both  many-core  CSCS  platforms  was  controlled  using  the  modules
framework which gives an easy  and flexible mechanism to access to all
of the  provided compilers, tools  and applications.  For  our initial
benchmarking,  we opted  for  the GNU  compiler  (gcc/4.8.1 on  Monch,
gcc/4.8.2 on Pilatus) with the -O3 compiler flag as it generally gives
a  good  level  of optimization  and  the  code  runs faster  in  this
configuration  than when  compiled with  the intel  compiler (14.0.1).
Besides, we installed the  MPICH2 implementation of MPI (mvapich2/1.9)
as  well  as  the  commonly  used HDF5  (1.8.12)  and  NetCDF  (4.3.1)
libraries in favor of the  traditional GRIB library for the management
of extremely  large and complex data collections.   All computes nodes
have    an   operating   system    based   on    GNU/Linux   featuring
``2.6.32-358.11.1.el6.x86\_64''      kernel      in     Monch      and
``3.0.101-0.15-default'' kernel in Pilatus.

\subsubsection{IBM BG/Q}
\textbf{THIS PART NEEDS TO BE COMPLETED. IT IS JUST A PLACEHOLDER}
All tests are compiled with the IBM~XL~C/\cpp compiler, with the options: {\ttfamily\scriptsize -qarch=qp -qtune=qp -O3}, and involve double-precision arithmetic.


\subsection{Run configuration}
\label{subsec:4.3}
A snapshot of the code, which includes, at least conceptually, all the
information needed  to reproduce the  energy-to-solution benchmarks of
COSMO-ART, was produced and run on a 1040 cores using 20 MPI tasks per
node on  Monch, on a  1344 cores using  16 MPI  tasks per  node on
Pilatus, and on 512 cores using 32 MPI tasks per node on the IBM BG/Q.
The  calculated  region  was mapped  to  the  participating
processors using  a 2-D partitioning strategy.   \textbf{MALOSSI: I suggest to remove the following part. It adds low level details that probably are not useful for the reader at this level.}
The distribution along
the~$x$ and~$y$~coordinates  was defined  by  setting: $nprocx=40$  and
$nprocy=26$ for  Monch and $nprocx=28$ and $nprocy=24$ \textbf{(for Pilatus?)} as $nprocx$ is
usually  kept  bigger  than  $nprocy$.   Besides as  this  version  of
COSMO-ART  doesn't  make  use   of  the  GRIB  library,  we  specified
$nprocio=0$ for  GRIB I/O.  Hyperthreading  is not considered  in this
study as previous  attempts of its use revealed that  it always led to
higher energy-to-solution \textbf{Also this last sentence is quite strong: I would remove it because we might reconsider this opportunity in a future stage of optimization}.\\

\subsection{Experimental results}
\label{subsec:4.4}

\begin{figure}[htbf]
  \begin{center}
    \includegraphics[width=0.48\textwidth]{Figs/NRJ_benchmark_Monch.eps}
    \caption{Monch: Isola E1 Rack 2 Total Power}
    \label{fig:1}
  \end{center}
\end{figure}

\begin{figure}[htbf]
  \begin{center}
    \includegraphics[width=0.48\textwidth]{Figs/NRJ_benchmark_Pilatus.eps}
    \caption{Pilatus: Isola HD Total Power}
    \label{fig:2}
  \end{center}
\end{figure}

Figures~\ref{fig:1} and  \ref{fig:2} account respectively  for Monch's
Isola E1  Rack 2  and Pilatus' Isola  HD total power  measurements for
1-day or 2-days simulations. On  the Intel Ivy Bridge EP based cluster
(i.e. Monch), the 1-day simulation  was issued only twice due to usage
restrictions. As time resolution was set to one update every 5 minutes
for  power sampling,  the average  power consumption  was  computed by
considering 6 values for each single  run.  On the Intel Xeon E5 based
cluster (i.e.   Pilatus), the 1-day  simulation was issued  four times
and a 2-days  run only once. Similarly, the  average power consumption
was computed by  considering 4 values for each single  1-day run and 9
values  for the  2-days  run. Corresponding  results  are gathered  in
Table~\ref{tab:3}.

\begin{table}[htbf]
  \begin{center}
    \caption{Average power consumption (W) of the platforms}
    \label{tab:3}
    \begin{tabular}{ccc}
      \hline\noalign{\smallskip}
      \textbf{Simulation time} & \textbf{Xeon E5} & \textbf{Ivy Bridge EP} \\
      \noalign{\smallskip}\hline\noalign{\smallskip}
      \textbf{1 day} & 18122.01417 & 12658.52278 \\
      & 17979.61083 & 12586.40833 \\
      & 18065.45167 & - \\
      & 17973.02833 & - \\
      \noalign{\smallskip}\hline\noalign{\smallskip}
      \textbf{2 days} & 17997.57815 & - \\
      \noalign{\smallskip}\hline
    \end{tabular}
  \end{center}
\end{table}

In Figure~\ref{fig:3}, we compare both time-to-solution (right y-axis)
and  energy-to-solution (left  y-axis) metrics  on both  platforms. As
expected,  Xeon  E5 outperforms  Ivy  Bridge  EP,  being roughly  1.3x
faster.  The reason for that is twofold: it has higher clock frequency
than Xeon E5 (2.6 GHz against  2.2 GHz) and it aims at computing speed
regardless to  energy consumption. In  our experiments, Ivy  Bridge EP
showed the best energy-to-solution, reducing the energy consumption of
Xeon E5 by approximately $7\%$.

\begin{figure}[htbf]
  \includegraphics[width=0.5\textwidth]{Figs/Time_E2S_COSMO-ART.eps}
  \caption{Time-to-solution and  energy-to-solution comparison between
    Xeon E5 and Ivy Bridge-EP architectures}
  \label{fig:3}
\end{figure}

