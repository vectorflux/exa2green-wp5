#! /bin/bash
#-----------------------------------------------------------------------------
# Version for Sun GridEngine
#
#$ -S /bin/bash
#$ -N job2
#$ -o sub$JOB_NAME.o$JOB_ID
#$ -j y
#$ -cwd
#$ -q serial
#
#-----------------------------------------------------------------------------
#
set -x
#
#-----------------------------------------------------------------------------
#
# setup required for using GridFTP in batch jobs
#
if [ "${HOSTNAME%[0-9]}" = "tornado" ]; then
    if [[ -z $WORK ]]; then
        export X509_USER_PROXY_DIR=${WORK}/$(id -un)/.proxy
        if [ ! -e $X509_USER_PROXY_DIR ]; then
            mkdir $X509_USER_PROXY_DIR
        fi
        export X509_USER_PROXY=${X509_USER_PROXY_DIR}/credential
    else
        echo environment variable WORK is unknown ; exit 1
    fi
fi
#
#-----------------------------------------------------------------------------
#
# this section gets parsed und updated by subroutine subjob during model run:
#
#DAY
#MONTH
#YEAR
#
#-----------------------------------------------------------------------------
#
# ok, now we can process the parsed model information:
# - the experiment id
EXP=xxx9999
#
base_dir=/scratch/work/mh0081/m214002/echam-5.5.00_rc1
#
# - the working directory
work_dir=$base_dir/experiments/${EXP}
#
# - the post processing directory
#
post_dir=$work_dir/means
#
# - the date:
day=$DAY
month=$MONTH
year=$YEAR
#
#-----------------------------------------------------------------------------
#
# the archive/mean directories:
#
archive_outdata_dir=gsiftp://gridftp.dkrz.de/prj/mh0081/arch/m214002/xxx9999/outdata
archive_means_dir=gsiftp://gridftp.dkrz.de/prj/mh0081/arch/m214002/xxx9999/means
#
#-----------------------------------------------------------------------------
#
# the directory for post processing will be created, if not already there
if [ ! -d $post_dir ]; then
    mkdir -p $post_dir
fi
#
#-----------------------------------------------------------------------------
#
# goto working directory
#
cd $work_dir
# 
# do the work
#
if [[ "${month}" = "12" ]]; then

# do default ECHAM postprocessing for one year
    
cat > input_list1 << EOF
 $select
 code=89,91,92,93,94,95,96,97,102,103,104,105,106,107,108,109,110,111,112,113,
      114,115,116,117,118,119,120,121,122,123,124,125,126,
      134,137,139,140,141,142,143,144,145,146,147,150,151,160,161,164,165,166,
      167,168,169,171,172,175,176,177,178,179,180,181,182,184,
      185,186,187,188,193,197,203,204,205,206,207,208,209,
      210,211,213,214,216,218,221,222,229,230,231,232,233,260,
 type=20,
 level=1,2,3,4,5,
 grib=1,
 mean=1,
 $end
EOF
#
cat > input_list2 << EOF
 &select
 code=130,131,132,133,153,154,156,157,223,
 level=100000,92500,85000,77500,70000,60000,50000,40000,
 30000,25000,20000,15000,10000,7000,5000,3000,1000,
 type=30,
 grib=1,
 mean=1
 &end
EOF
#
cat > input_list3 << EOF
 &select
 code=138,148,149,155,
 level=100000,92500,85000,77500,70000,60000,50000,40000,
 30000,25000,20000,15000,10000,7000,5000,3000,1000,
 type=70,
 grib=1,
 mean=1
 &end
EOF
#---
    # GridFTP (needs URL for local files):
    source_dir="file://$work_dir"

    for m in 01 02 03 04 05 06 07 08 09 10 11 12 ; do

	input_file=${EXP}_${year}${m}.${day}

        # make post processing

	/client/bin/after < input_list1 $input_file $post_dir/BOT_${year}${m}
	/client/bin/after < input_list2 $input_file ATM_1
	/client/bin/after < input_list3 $input_file ATM_2

	/client/bin/cdo merge ATM_1 ATM_2 $post_dir/ATM_${year}${m}

        # save files month by month (recommended for high resolution)

        globus-url-copy -vb -p 10 \
	    $source_dir/$input_file \
	    $archive_outdata_dir/$input_file

   done

#   # save files per year (recommended for low resolutions)
#
#   input_file=${EXP}_${year}.grb
#   /client/bin/cdo cat ${EXP}_${year}*.day $input_file
#
#   globus-url-copy -vb -p 10 \
#       $source_dir/$input_file \
#       $archive_outdata_dir/$input_file
#
   # save monthly means

    cd $post_dir

    /client/bin/cdo cat BOT_${year}* BOT_${year}
    /client/bin/cdo cat ATM_${year}* ATM_${year}

    # GridFTP (needs URL for local files):
    source_dir="file://$post_dir"

    # no error checking!    
    globus-url-copy -vb -p 10 \
	$source_dir/BOT_${year} \
	$archive_means_dir/BOT_${year}

    # no error checking!    
    globus-url-copy -vb -p 10 \
	$source_dir/ATM_${year} \
	$archive_means_dir/ATM_${year}

fi
#
#-----------------------------------------------------------------------------
#
exit
#
#-----------------------------------------------------------------------------
